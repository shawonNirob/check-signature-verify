{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1444b44a-52b9-4bf4-88b1-3b3d50a30027",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/shawon/.pyenv/versions/3.10.13/lib/python3.10/site-packages (2.7.0)\n",
      "Requirement already satisfied: torchvision in /Users/shawon/.pyenv/versions/3.10.13/lib/python3.10/site-packages (0.22.0)\n",
      "Requirement already satisfied: transformers in /Users/shawon/.pyenv/versions/3.10.13/lib/python3.10/site-packages (4.52.1)\n",
      "Collecting timm\n",
      "  Downloading timm-1.0.15-py3-none-any.whl.metadata (52 kB)\n",
      "Requirement already satisfied: filelock in /Users/shawon/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/shawon/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/shawon/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/shawon/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/shawon/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/shawon/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from torch) (2025.5.0)\n",
      "Requirement already satisfied: numpy in /Users/shawon/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/shawon/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/shawon/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from transformers) (0.31.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/shawon/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/shawon/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/shawon/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/shawon/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/shawon/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/shawon/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/shawon/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/shawon/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/shawon/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shawon/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shawon/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shawon/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shawon/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from requests->transformers) (2025.4.26)\n",
      "Downloading timm-1.0.15-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: timm\n",
      "Successfully installed timm-1.0.15\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision transformers timm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e1fcfae-b6fb-4f82-9d76-d5072c21bbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shawon/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from transformers import DeiTModel, DeiTConfig\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class ResNetDeiTHybrid(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetDeiTHybrid, self).__init__()\n",
    "        \n",
    "        # Load pretrained ResNet-50 and remove final layers\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet_backbone = nn.Sequential(*list(resnet.children())[:-2])  # Output: [B, 2048, 7, 7]\n",
    "        \n",
    "        # Flatten conv features to tokens\n",
    "        self.proj_to_vit = nn.Conv2d(2048, 768, kernel_size=1)  # Match DeiT hidden size\n",
    "        \n",
    "        # DeiT Transformer (without the patch embedding)\n",
    "        config = DeiTConfig.from_pretrained(\"facebook/deit-base-distilled-patch16-224\")\n",
    "        self.transformer = DeiTModel(config)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [B, 3, 224, 224]\n",
    "        conv_feats = self.resnet_backbone(x)            # [B, 2048, 7, 7]\n",
    "        tokens = self.proj_to_vit(conv_feats)           # [B, 768, 7, 7]\n",
    "        tokens = tokens.flatten(2).transpose(1, 2)      # [B, 49, 768]\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        cls_token = self.transformer.embeddings.cls_token.expand(batch_size, -1, -1)  # [B, 1, 768]\n",
    "        tokens = torch.cat((cls_token, tokens), dim=1)                                # [B, 50, 768]\n",
    "\n",
    "        # Add positional embeddings\n",
    "        tokens = tokens + self.transformer.embeddings.position_embeddings[:, :tokens.size(1), :]\n",
    "        tokens = self.transformer.encoder(tokens).last_hidden_state\n",
    "        \n",
    "        cls_output = tokens[:, 0]  # [CLS] token\n",
    "        return cls_output\n",
    "\n",
    "def extract_vector(image_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    hybrid_model = ResNetDeiTHybrid().to(device)\n",
    "    hybrid_model.eval()\n",
    "\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    return get_vector(img, hybrid_model, device)\n",
    "        \n",
    "def get_vector(image_pil, model, device):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "    ])\n",
    "    image_tensor = preprocess(image_pil).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedding = model(image_tensor)\n",
    "    \n",
    "    vector = embedding.squeeze().cpu().numpy()\n",
    "    vector /= np.linalg.norm(vector)\n",
    "    return vector\n",
    "    \n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return float(np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2)))\n",
    "\n",
    "def get_similarity(check_signature, stored_signature):\n",
    "    vec1 = extract_vector(check_signature)\n",
    "    vec2 = extract_vector(stored_signature)\n",
    "    similarity = cosine_similarity(vec1, vec2)\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74ba8955-9084-4dd6-8474-5f922858d16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signature_vector(image_pil, model, device):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "    ])\n",
    "    image_tensor = preprocess(image_pil).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedding = model(image_tensor)\n",
    "    \n",
    "    vector = embedding.squeeze().cpu().numpy()\n",
    "    vector /= np.linalg.norm(vector)\n",
    "    return vector\n",
    "    \n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return float(np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1900c878-ed5d-46c1-b417-f2120fd5e9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 0.9712\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hybrid_model = ResNetDeiTHybrid().to(device)\n",
    "hybrid_model.eval()\n",
    "\n",
    "img1 = Image.open(\"check_signature_test_case/process_signature/anik_2.jpeg\").convert(\"RGB\")\n",
    "img2 = Image.open(\"check_signature_test_case/process_signature/anik_1.jpeg\").convert(\"RGB\")\n",
    "\n",
    "vec1 = get_signature_vector(img1, hybrid_model, device)\n",
    "vec2 = get_signature_vector(img2, hybrid_model, device)\n",
    "\n",
    "similarity = cosine_similarity(vec1, vec2)\n",
    "print(f\"Similarity Score: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d0851f4-f823-44cc-900c-6682a7ab1547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def isolate_signature(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Threshold to binary image (signature in white, background in black)\n",
    "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Create empty mask to draw filtered contours\n",
    "    mask = np.zeros_like(binary)\n",
    "\n",
    "    # Filter and draw only meaningful signature-like contours\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "\n",
    "        # Filter conditions\n",
    "        if area > 100 and w > 10 and h > 10:  # Skip tiny specks\n",
    "            mask = cv2.drawContours(mask, [cnt], -1, 255, -1)  # Fill the contour\n",
    "\n",
    "    # Final result (bitwise AND to extract the denoised signature)\n",
    "    result = cv2.bitwise_and(binary, mask)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Load model and image\n",
    "def process_signature_image(image_path):\n",
    "    model = YOLO(\"yolov8s.pt\")\n",
    "    image = cv2.imread(image_path)\n",
    "    results = model(image)\n",
    "\n",
    "    if results[0].boxes is None or len(results[0].boxes) == 0:\n",
    "        print(\"No detection from yolov8s\")\n",
    "        return None\n",
    "\n",
    "    processed_images = []\n",
    "    for i, box in enumerate(results[0].boxes):\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "        cropped = image[y1:y2, x1:x2]\n",
    "        cleaned = isolate_signature(cropped)\n",
    "        processed_images.append(cleaned)\n",
    "\n",
    "    return processed_images  # List of processed images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2e742ed-2758-43d4-8cdd-6844beca54cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     uploaded_file\u001b[38;5;241m.\u001b[39msave(save_path)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m save_path\n\u001b[0;32m---> 19\u001b[0m save_image1 \u001b[38;5;241m=\u001b[39m \u001b[43msave_uploaded_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheck_signature_test_case/signature/nasren_1.jpeg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m save_image2 \u001b[38;5;241m=\u001b[39m save_uploaded_file(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheck_signature_test_case/signature/nasren_2.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m process_image1 \u001b[38;5;241m=\u001b[39m process_signature_image(save_image1)\n",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m, in \u001b[0;36msave_uploaded_file\u001b[0;34m(uploaded_file, save_dir)\u001b[0m\n\u001b[1;32m     14\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muploaded_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m save_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, filename)\n\u001b[0;32m---> 16\u001b[0m \u001b[43muploaded_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m(save_path)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m save_path\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'save'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3954e5-acae-4556-aef2-2988e03a00f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
